<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  <!-- Favicon -->
  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <!-- Base stylings -->
  <link rel="stylesheet" href="/styles/base.css">

  <!-- Analytics -->
  <script data-goatcounter="https://mlu.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

  <!-- Block for title, css, scripts, etc.-->
  
<title>Proving the No-Free-Lunch Theorem</title>
<link rel="stylesheet" href="/styles/post.css">
<link rel="stylesheet" href="/styles/muse_post.css">
<script defer src="https://my.remarkbox.com/static/js/iframe-resizer/iframeResizer.min.js"></script>
<script defer src="/scripts/remarkbox.js"></script>

</head>

<body>
  
  
<div class="top_bar">
    <div class="logo-holder">
      <a href="/muse/">
        <img src="/images/muse_logo.svg" class="logo">
      </a>
    </div>
  </div>


  <div class="grid">
    <!-- <div class="grid_spacer"></div> -->
    <!-- Block for content -->
    <div class="container">
      
<div class="post-title">Proving the No-Free-Lunch Theorem</div>
<div class="content"><p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"></p>

<p><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script></p>

<p>The No-Free-Lunch Theorem is an important part of statistical learning theory. I&#39;m going to trace the proof found in <em>Understanding Machine Learning</em> for my own benefit.</p>

<p>The theorem is this:</p>

<p>Say there is a learning algorithm A which outputs a binary classifier and has the 0-1 loss function over a domain \(X\). Then, there exists a distribution \(D\) over \(X \times \{0,1\}\) such that:</p>

<ol>
<li><p>There exists a function \(f: X \to \{0,1\}​\) with \(L_D(f) = 0​\). That is, there is some binary function over \(X​\) such that the expected loss of \(f​\) over \(D​\) is 0, meaning there is some function which will correctly classify all the points.</p></li>
<li><p>With probability of at least \(\frac{1}{7}\) over the choice of \(S \sim D^m\) \(L_D(A(S)) \ge \frac{1}{8}\). In other words, for at least \(\frac{1}{7}\) of all possible training sets of size \(m\) from \(D\), the expected loss of output of the training algorithm on the training set is at least  \(\frac{1}{8}\).</p></li>
</ol>

<h3>Proof:</h3>

<p>Let \(C \subseteq X\) such that \(|C| = 2m\). We note that there are \(T = 2^{2m}\) possible functions from \(C\) to \(\{0,1\}\).  Let \(D_i\) be the distribution over \(D\) that only assigns probability to the \((x,f_i(x))\) pairs for the corresponding function.</p>

<p>We&#39;re going to show that:</p>

<p>\(\underset{i \in [T]}{\text{max }} \underset{S \sim D_i^m}{\mathbb{E}} [L_{D_i}(A(S))] \ge 1/4\)</p>

<p>That is, the largest expected loss of running our learning algorithm on a training set of size \(m\) among all functions from \(C\) to \(\{0,1\}\) (or rather, the distribution that is defined only on the function&#39;s \((x, f(x))\) pairs) is 1/4.</p>

<p>When a random variable is between 0 and 1, we can use Markov&#39;s inequality to show that:</p>

<p>\(P(X \ge a) \ge \frac{\mathbb{E}[X]-a}{1-a}\)</p>

<p>Applying that to our inequality above, we see that:</p>

<p>\(P(L_D(A&#39;(S)) \ge \frac{1}{8}) \ge \frac{\frac{1}{4}-\frac{1}{8}}{\frac{7}{8}} \ge \frac{1}{7}\) as desired.</p>

<p>So now we just have to prove the inequality itself.</p>

<p>Well, what do we know about the expected loss for a given distribution and a learning algorithm? It&#39;s just the averaged loss, over all possible choices of a training set:</p>

<p>\(\underset{S \sim D_i^m}{\mathbb{E}}[L_{D_i}(A(S))] = \frac{1}{k}\sum_{j=1}^kL_{D_i}(A(S^i_j))\)</p>

<p>And we care about taking the maximum among all possible distributions, which is greater than the average loss among all distributions, which is greater than the minimum value:</p>

<p>\(\underset{i \in [T]}{\text{max }} \frac{1}{k}\sum_{j=1}^kL_{D_i}(A(S^i_j))\) </p>

<p>\(\ge \frac{1}{T}\sum^T_{i=1}\frac{1}{k}\sum^k_{j=1}L_{D_i}(A(S^i_j))​\)</p>

<p>\(=\frac{1}{k}\sum^k_{j=1}\frac{1}{T}\sum^T_{i=1}L_{D_i}(A(S^i_j))​\)</p>

<p>\(\ge \underset{j \in [k]}{\text{min }} \frac{1}{T}\sum_{i=1}^TL_{D_i}(A(S^i_j))\)</p>

<p>(To be clear, the last inequality refers to the lowest cost possible among all possible training sets, averaged across all possible distributions.)</p>

<p>Next, let&#39;s fix some \(j \in [k]\). \(S_j = (x_1,..,x_m)\). Let \((v_1, ...,v_p)\) be the examples in \(C\) that aren&#39;t in \(S_j\). We can see that \(p \ge m\).</p>

<p>Thus:</p>

<p>\(L_{D_i}(h) = \frac{1}{2m}\sum_{x\in C}\mathbf{1}[h(x) \not= f_i(x)]\)</p>

<p>\(\ge \frac{1}{2p}\sum_{j=1}^p\mathbf{1}[h(v_j) \not= f_i(v_j)]\)</p>

<p>Combining our two inequalities, we get:</p>

<p>\( \frac{1}{T}\sum_{i=1}^TL_{D_i}(A(S^i_j)) \ge \frac{1}{T}\sum_{i=1}^T \frac{1}{2p}\sum_{j=1}^p\mathbf{1}[h(v_j) \not= f_i(v_j)]\)</p>

<p>\( = \frac{1}{2p}\sum_{j=1}^p\frac{1}{T}\sum_{i=1}^T \mathbf{1}[h(v_j) \not= f_i(v_j)]\)</p>

<p>\(\ge \frac{1}{2}\underset{r \in [p]}{\text{min}}\frac{1}{T}\sum_{i=1}^T \mathbf{1}[h(v_j) \not= f_i(v_j)]\)</p>

<p>Now, fix a point \(v_r \in [p]\). Notice that we can partition \([T]\) into \((f_a, f_b)\) pairs such that \(f_a\) and \(f_b\) agree on every value except for \(v_r\). We can do this because \([T]\) is the set of all functions, so we can imagine fixing \(f(v_r) = 0\) and letting all the other values vary, as well as fixing \(f(v_r) = 1\) and doing the same thing. It&#39;s clear this will give us two sets of equal size, as well as enable the above pairing.</p>

<p>This means that:</p>

<p>\(\mathbf{1}[h(v_j) \not = f_a(j)] + \mathbf{1}[h(v_j) \not = f_b(j)] = 1\), which means that the average loss for the pair of functions is \(\frac{1}{2}\).</p>

<p>Thus, \(\frac{1}{2}\underset{r \in [p]}{\text{min}}\frac{1}{T}\sum_{i=1}^T \mathbf{1}[h(v_j) \not= f_i(v_j)] = \frac{1}{4}\)</p>

<p>Thus, \(\underset{i \in [T]}{\text{max }} \frac{1}{k}\sum_{j=1}^kL_{D_i}(A(S^i_j)) \ge \frac{1}{4}\) as desired.</p>
</div>
<div class="date">Last Updated: 2020-05-25 11:35</div>

<hr class="comment-spacer">
<div id="remarkbox-div">
  <noscript>
    <iframe id=remarkbox-iframe src="https://my.remarkbox.com/embed?nojs=true" style="height:600px;width:100%;border:none!important" tabindex=0></iframe>
  </noscript>
</div>

    </div>
    <!-- <div class="grid_spacer"></div> -->
  </div>
</body>

</html>