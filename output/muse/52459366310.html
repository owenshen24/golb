<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  <!-- Favicon -->
  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <!-- Base stylings -->
  <link rel="stylesheet" href="/styles/base.css">

  <!-- Analytics -->
  <script data-goatcounter="https://mlu.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

  <!-- Block for title, css, scripts, etc.-->
  
<title>Mathematics for Machine Learning</title>
<link rel="stylesheet" href="/styles/muse_post.css">
<script defer src="https://my.remarkbox.com/static/js/iframe-resizer/iframeResizer.min.js"></script>
<script defer src="/scripts/remarkbox.js"></script>

</head>

<body>
  
  
<div class="top_bar">
    <div class="logo-holder">
      <a href="/muse/">
        <img src="/images/muse_logo.svg" class="logo">
      </a>
    </div>
  </div>


  <div class="grid">
    <!-- <div class="grid_spacer"></div> -->
    <!-- Block for content -->
    <div class="container">
      
<div class="post-title">Mathematics for Machine Learning</div>
<div class="content"><p>I need to brush up on my formal understanding as well as the intuitions behind the maths in machine learning. To that end, I&#39;ll be working through exercises in Mathematics for Machine Learning as well as summarizing chapters.</p>

<h1>Assorted Notes:</h1>

<ul>
<li><p>Hessians and the 2nd degree Taylor expansion:</p>

<ul>
<li>Taylor&#39;s theorem tells us how we can approximate a differentiable function.</li>
<li>More specifically, as the Hessian represents the matrix of second derivatives, we&#39;re typically talking about quadratic approximations.</li>
<li>Specifically: \(f(a) + f&#39;(a)(x-a) + \frac{f&#39;&#39;(a)}{2}(x-a)^2\).</li>
<li>Ah, furthermore, as the Hessian can be used in the 2nd derivative test, we know that if the Hessian is positive definite, then we&#39;re at a local minima.</li>
</ul></li>
<li><p>Hessians, diagonalization, and eigenvectors:</p></li>
</ul>
</div>
<div class="date">Last Updated: 2019-10-24 15:37</div>

<hr class="comment-spacer">
<div id="remarkbox-div">
  <noscript>
    <iframe id=remarkbox-iframe src="https://my.remarkbox.com/embed?nojs=true" style="height:600px;width:100%;border:none!important" tabindex=0></iframe>
  </noscript>
</div>

    </div>
    <!-- <div class="grid_spacer"></div> -->
  </div>
</body>

</html>