<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="icon" href="/images/favicon.ico" type="image/x-icon">
<link rel="preload" href="/styles/fonts/hind-v9-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
<link rel="stylesheet" href="/styles/base.css">
<script data-goatcounter="https://mlu.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<title>Interpretability through insight compression</title>
<link rel="stylesheet" href="/styles/post.css">
<link rel="stylesheet" href="/styles/muse_post.css">
<script defer="defer" src="https://my.remarkbox.com/static/js/iframe-resizer/iframeResizer.min.js"></script>
<script defer="defer" src="/scripts/remarkbox.js"></script>
</head>
<body>
<div class="top_bar">
<div class="logo-holder">
<a href="/muse/">
<img src="/images/muse_logo.svg" class="logo">
</a>
</div>
</div>
<div class="grid">
<div class="container">
<div class="post-title">Interpretability through insight compression</div>
<div class="content"><p>In interpretable machine learning, I think one important question we should be able to ask of our models is “What is something useful you can tell me about your understanding of the data?” Much of the current research lacks agreement on a standardized definition, and existing methods are often subjectively evaluated (e.g. “It just looks better”). I don’t expect the following definition to be useful everywhere, but I think it opens up a set of questions that haven’t been explored yet.</p>
<p>Ideally, we want our models to be useful outside of just their black-box output; one way to extract more value and potentially learn more about how they operate is to impose the additional constraint that they output something useful, i.e. an “insight”, to the human operator. The insight should be something that helps our own performance at the task.</p>
<p>A simple way to formalize this in the typical machine learning paradigm is to set up a dual-training loop where a powerful model is optimized to both minimize loss, as well as output some bits that optimize a weaker model (which approximates a human). A set of concrete steps I’d take might look like:</p>
<ol>
<li>Determine a reasonable toy task as proof of concept</li>
<li>Think about to what extent the toy task can generalize</li>
<li>Find a way to represent human judgment via a model</li>
<li>The difficulty here will be encoding the model in a way that we can then port the insights back into human ontology.</li>
<li>Set up the actual training loop in PyTorch and run the code.</li>
<li>Inspect the final output model’s insights.</li>
</ol>
<p>I think this idea is mostly based on how successful I can be at evaluating the usefulness of the additional bits as insight at scale, in a way that approximates how a human might actually respond. The main concern I have with this is that the simple training setup as described above could end up just being a proxy optimizer for the smaller model. A negative outcome would be if this ends up being an example of mesa-optimization for the larger model. I think this could still be useful, even in such a case, as it provides a small-scale example of mesa-optimization in practice.</p>
</div>
<hr class="comment-spacer">
<div class="date">Last Updated: 2020-09-19 13:42</div>
<div class="date">First Published: 2020-09-19 13:42</div>
<div id="remarkbox-div">
<noscript>
<iframe id="remarkbox-iframe" src="https://my.remarkbox.com/embed?nojs=true" style="height:600px;width:100%;border:none!important" tabindex="0"></iframe>
</noscript>
</div>
</div>
</div>
</body>
</html>